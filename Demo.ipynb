{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CC4MCSLAEnriched\n",
    "\n",
    "In this notebook, we present our approach implementation. Our approach is based on three components, first the annotation, second the abstraction and last the Checker. \n",
    "\n",
    "## Annotation ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the ontology using owlready2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import * \n",
    "onto = get_ontology('http://test.org/onto.owl')\n",
    "\n",
    "##### Ontology Definition #####\n",
    "with onto: \n",
    "    class stateMachine(Thing):\n",
    "        pass\n",
    "    class State(stateMachine):\n",
    "        pass\n",
    "    class Start(State):\n",
    "        pass\n",
    "    class Execute(State):\n",
    "        pass\n",
    "    class Complete(State):\n",
    "        pass\n",
    "    class Transition(stateMachine):\n",
    "        pass\n",
    "    class eventType(Thing):\n",
    "        pass\n",
    "    class isRelatedTo(ObjectProperty, FunctionalProperty):\n",
    "        domain = [eventType]\n",
    "        range  = [stateMachine]\n",
    "\n",
    "##### Declare event types\n",
    "Service_Create = eventType('Service_Create', isRelatedTo=Start)\n",
    "Service_Remove = eventType('Service_Remove', isRelatedTo=Start)\n",
    "Service_Update = eventType('Service_Update', isRelatedTo=Start)\n",
    "Container_Create = eventType('Container_Create', isRelatedTo=Execute)\n",
    "Container_Destroy = eventType('Container_Destroy', isRelatedTo=Execute)\n",
    "Container_Start = eventType('Container_Start', isRelatedTo=Complete)\n",
    "Container_Stop = eventType('Container_Stop', isRelatedTo=Complete)\n",
    "Ressource_Usage = eventType('Ressource_Usage', isRelatedTo=Transition)\n",
    "\n",
    "onto.save(file='onto.owl', format=\"rdfxml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a function for finding ancestors of indentified event type in event logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Ancestors Calling #####\n",
    "def search_ancestors(onto, ask):\n",
    "    result = onto.search(iri = \"*{}\".format(ask))\n",
    "    lcStep = str(result[0].isRelatedTo).split('.')[1]\n",
    "    smElt = str(result[0].isRelatedTo.is_a[0]).split('.')[1]\n",
    "    if lcStep == 'Transition':\n",
    "        smElt = 'Transition'\n",
    "        lcStep = 'N/A'\n",
    "    return [smElt, lcStep]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we implement the annotation function of xes format adding the state-machine element and lifecycle step for each event and return an xes file with the annotated events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmechouch\\AppData\\Local\\Temp\\ipykernel_9736\\2391464443.py:7: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  dataframe = pm4py.format_dataframe(dataframe, case_id='Resource Name', activity_key='Event-Type', timestamp_key='Timestamp')\n",
      "c:\\Users\\jmechouch\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pm4py\\utils.py:486: UserWarning: the EventLog class has been deprecated and will be removed in a future release.\n",
      "  warnings.warn(\"the EventLog class has been deprecated and will be removed in a future release.\")\n",
      "c:\\Users\\jmechouch\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "exporting log, completed traces :: 100%|██████████| 3/3 [00:00<00:00, 515.80it/s]\n"
     ]
    }
   ],
   "source": [
    "#### Pre-processing based on ontology ####\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "\n",
    "#### Import event-logs from\n",
    "dataframe = pd.read_csv('logs.csv', sep=',')\n",
    "dataframe = pm4py.format_dataframe(dataframe, case_id='Resource Name', activity_key='Event-Type', timestamp_key='Timestamp')\n",
    "\n",
    "#### Iterate through event logs ####\n",
    "for idx, row in dataframe.iterrows():\n",
    "    # Search event type in ontology and returns ancestors \n",
    "    smElt, lcStep = search_ancestors(onto, row['Event-Type'])\n",
    "    dataframe.loc[[idx],'smElt'] = smElt\n",
    "    dataframe.loc[[idx],'lcStep'] = lcStep\n",
    "\n",
    "### Export as XES ###\n",
    "event_log = pm4py.convert_to_event_log(dataframe)\n",
    "xes = pm4py.write_xes(event_log, 'exported.xes')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstraction\n",
    "\n",
    "Based on the annotated event logs, we abstract state-machine using our defined patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 3/3 [00:00<00:00, 442.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State abstraction : Pattern 3.1\n",
      "State-Type abstraction : Pattern 3.2\n",
      "Transition abstraction : Pattern 3.4\n",
      "State abstraction : Pattern 3.1\n",
      "State-Type abstraction : Pattern 3.2\n",
      "Transition abstraction : Pattern 3.4\n",
      "State abstraction : Pattern 3.1\n",
      "State-Type abstraction : Pattern 3.2\n",
      "Transition abstraction : Pattern 3.4\n"
     ]
    }
   ],
   "source": [
    "import pm4py\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "from StateMachine import StateMachine\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "def pattern_identification(log: pd.DataFrame, pattern: List, attribute: str):\n",
    "    \"\"\" \n",
    "        Return index of pattern in log in DataFrame \n",
    "        To Do: \n",
    "            - Add functionnalities to define patterns across several attributes\n",
    "            - Enabled possibilities of eventually follows pattern and several states\n",
    "    \"\"\"\n",
    "    # Identify number of item in pattern\n",
    "    nbPattern = len(pattern)\n",
    "\n",
    "    # Construction pattern as string\n",
    "    ## Begin of request\n",
    "    pattern_s = f\"\"\"log.index[(log['{attribute}'] == '{pattern[0]}')\"\"\"\n",
    "    for item in range(1, nbPattern):\n",
    "        pattern_s += f\"\"\" & (log['{attribute}'].shift(-{item}) == '{pattern[item]}')\"\"\"\n",
    "    ## End of request\n",
    "    pattern_s += f\"\"\"]\"\"\"\n",
    "\n",
    "    # Execution of defined pattern\n",
    "    indice_p_s = eval(pattern_s)\n",
    "    return indice_p_s\n",
    "\n",
    "def state_abstraction(log: pd.DataFrame):\n",
    "    \"\"\"\n",
    "        Return states identified\n",
    "    \"\"\"\n",
    "    ## Declare discovered state machine\n",
    "    SM_Discovered = StateMachine(\n",
    "        name=''\n",
    "    )\n",
    "    pattern=['Start', 'Execute', 'Complete']\n",
    "\n",
    "    states_index = pattern_identification(log, pattern, 'lcStep')\n",
    "    states_name = []\n",
    "\n",
    "    for i, s in enumerate(states_index, 1):\n",
    "        S_name = 'S'+str(i)\n",
    "        states_name.append(S_name)\n",
    "        SM_Discovered.add_state(StateMachine.state(\n",
    "            name= S_name,\n",
    "            type='',\n",
    "            Resourcerequirements={\n",
    "                log.loc[s]['Metric'] : log.loc[s]['Value']\n",
    "            }\n",
    "        ))\n",
    "    return SM_Discovered, states_index, states_name\n",
    "\n",
    "def state_type_abstraction(log: pd.DataFrame, State_Machine_Discovered: StateMachine):\n",
    "    \"\"\"\n",
    "        Apply State-Type Abstraction\n",
    "    \"\"\"\n",
    "    state_nb = len(State_Machine_Discovered.states)\n",
    "    for state in State_Machine_Discovered.states:\n",
    "        if state.name == 'S1':\n",
    "            state.set_type('isInitial')\n",
    "        elif int(state.name[1:]) < state_nb:\n",
    "            state.set_type('isNormal')\n",
    "        elif int(state.name[1:]) == state_nb:\n",
    "            state.set_type('isFinal')\n",
    "    return State_Machine_Discovered\n",
    "\n",
    "def transition_abstraction(log: pd.DataFrame, State_Machine_Discovered: StateMachine, states_index):\n",
    "    \"\"\"\n",
    "        Abstraction transition by combining reconfiguration actions and triggering event associated to the state-machine\n",
    "\n",
    "    \"\"\"\n",
    "    states = State_Machine_Discovered.states\n",
    "    for idx, state in enumerate(states):\n",
    "        if idx < (len(states) - 1):\n",
    "            diff_state = int(states[idx+1].Resourcerequirements['replicas']) - int(states[idx].Resourcerequirements['replicas'])\n",
    "\n",
    "            if diff_state > 0:\n",
    "                type = 'Scale-out'\n",
    "            elif diff_state < 0:\n",
    "                type = 'Scale-in'\n",
    "            else:\n",
    "                type = 'Error'\n",
    "                print('Error: State Equivalent')\n",
    "\n",
    "            #### Get states Event\n",
    "            # Set time window selected\n",
    "            time_window = timedelta(minutes=1)\n",
    "\n",
    "            # Select events in the time window before state execution\n",
    "            pattern_ts = log.loc[states_index[idx+1]]['time:timestamp']\n",
    "            pattern_ts_minus_tw = (pattern_ts - time_window).isoformat()\n",
    "            transition_Window = log[ ( log['time:timestamp'] > pattern_ts_minus_tw) & \\\n",
    "                (log['time:timestamp'] < pattern_ts) & (log['smElt'] == 'Transition' )].astype({'Value': int})\n",
    "            \n",
    "            # Return for each metric observed a consumption average\n",
    "            avg = transition_Window.groupby('Metric')['Value'].mean().to_dict()\n",
    "\n",
    "            if bool(avg) != False:\n",
    "                State_Machine_Discovered.add_transition(\n",
    "                    StateMachine.transition(\n",
    "                        name=f\"T{idx+1}\",\n",
    "                        source=state.name,\n",
    "                        target=states[idx+1].name,\n",
    "                        events=[StateMachine.event(\n",
    "                                    id = 'E1',\n",
    "                                    type = 'ResourceRelatedEvent',\n",
    "                                    predicate = {\n",
    "                                        'metric': 'Cpu Usage',\n",
    "                                        'operator': '>=',\n",
    "                                        'refValue': avg['Cpu Usage'],\n",
    "                                        'time': str(time_window.total_seconds()) + 's'\n",
    "                                    })],\n",
    "                        actions=[StateMachine.action(\n",
    "                            id = 'A1',\n",
    "                            type = type,\n",
    "                            attributes= {\n",
    "                                'replicas' : abs(diff_state)\n",
    "                            }\n",
    "                        )]\n",
    "                ))\n",
    "\n",
    "    return State_Machine_Discovered\n",
    "\n",
    "##### Importation of annotated event logs #####\n",
    "file_path = 'exported.xes'\n",
    "event_log = pm4py.read_xes(file_path)\n",
    "\n",
    "# Filter by case\n",
    "events = event_log.groupby('@@case_index')\n",
    "for i, case_event_log in events:\n",
    "    ##### State abstraction : Pattern 3.1 #####\n",
    "    print(\"State abstraction : Pattern 3.1\")\n",
    "    SM_Discovered, states_index, states_name = state_abstraction(case_event_log)\n",
    "    \n",
    "    ##### State-Type abstraction : Pattern 3.2 #####\n",
    "    print(\"State-Type abstraction : Pattern 3.2\")\n",
    "    SM_Discovered = state_type_abstraction(case_event_log, SM_Discovered)\n",
    "\n",
    "    ##### Transition abstraction : Pattern 3.3 + 3.4 #####\n",
    "    print(\"Transition abstraction : Pattern 3.4\")\n",
    "    SM_Discovered = transition_abstraction(case_event_log, SM_Discovered, states_index)\n",
    "\n",
    "    json = SM_Discovered.to_json()\n",
    "\n",
    "    with open(\"SM_discovered/SM_.json\", \"w\") as outfile:\n",
    "        outfile.write(json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report : \n",
      "Path : [([0, \"['S1', 'S1']\", \"['S2', 'S2']\", \"['S3', 'S3']\"], 3.3)]\n",
      "Y_Optimal : 3.3\n",
      "FitnessValue : 0.89\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Checker \n",
    "\"\"\" \n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "def to_graph(SM):\n",
    "    SM_disc = nx.DiGraph()\n",
    "    for state in SM['_StateMachine__states']:\n",
    "        SM_disc.add_node(state['_state__name'], replicas = state['_state__Resourcerequirements']['replicas'], type=state['_state__type'])\n",
    "    for transition in SM['_StateMachine__transitions']:\n",
    "        SM_disc.add_edge(transition['_transition__source'], transition['_transition__target'], \n",
    "                         name=transition['_transition__name'], events=transition['_transition__events'], actions=transition['_transition__actions'])\n",
    "    return SM_disc\n",
    "\n",
    "def get_initial_nodes(graph):\n",
    "    return [n for n,d in graph.in_degree() if d==0]\n",
    "\n",
    "def get_final_nodes(graph):\n",
    "    return [n for n,d in graph.out_degree() if d==0]\n",
    "\n",
    "### Get Discovered state-machine\n",
    "SM_Disc = to_graph(json.load(open(\"SM_discovered/SM_.json\")))\n",
    "\n",
    "### Get Defined state-machine\n",
    "SM_Def = to_graph(json.load(open(\"SM_Defined.json\")))\n",
    "\n",
    "### Search Space construction\n",
    "SS = nx.DiGraph()\n",
    "SS.add_node(0, weight=0)\n",
    "\n",
    "for i, (eltx, elty) in enumerate(zip(SM_Disc.nodes, SM_Def.nodes)):\n",
    "    temp_last_nodes = get_final_nodes(SS)\n",
    "    # e = epsilon to guarantee end\n",
    "    e = i*0.1\n",
    "    if SM_Disc.nodes[eltx]['replicas'] == SM_Def.nodes[eltx]['replicas']:\n",
    "        # State equivalent \n",
    "        SS.add_node(str([eltx,elty]), weight=1+e)\n",
    "        [SS.add_edge(node, str([eltx,elty])) for node in temp_last_nodes]\n",
    "    else:\n",
    "        SS.add_node(str([eltx,'>>']), weight=5+e)\n",
    "        SS.add_node(str(['>>',elty]), weight=5+e)\n",
    "        [SS.add_edge(node, str([eltx,'>>'])) for node in temp_last_nodes]\n",
    "        [SS.add_edge(node, str(['>>',elty])) for node in temp_last_nodes]\n",
    "\n",
    "### Identify starting and ending nodes of the search space\n",
    "starting_nodes = get_initial_nodes(SS)\n",
    "ending_nodes = get_final_nodes(SS)\n",
    "\n",
    "# Compute the worst possible alignment\n",
    "y_worst_sum = ((len(SM_Def.nodes) * 2 ) * 5)\n",
    "\n",
    "### Compute the cost of an identified alignment \n",
    "results_path = []\n",
    "for s in starting_nodes:\n",
    "    for e in ending_nodes:\n",
    "        y_optimal_cost = 0\n",
    "        path = nx.astar_path(SS, s, e)\n",
    "        for elt in path: y_optimal_cost+=SS.nodes[elt]['weight']\n",
    "        results_path.append((path, y_optimal_cost))\n",
    "        fitnessValue = 1 - y_optimal_cost / y_worst_sum\n",
    "\n",
    "print(\"Report : \")\n",
    "print(f\"Path : {results_path}\")\n",
    "print(f\"Y_Optimal : {y_optimal_cost}\")\n",
    "print(f\"FitnessValue : {fitnessValue}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d1ea9fb40c093137e5631ea40405a634864414780e4b5725c09d6f101604b5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
