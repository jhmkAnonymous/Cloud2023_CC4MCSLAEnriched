{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CC4MCSLAEnriched\n",
    "\n",
    "The notebook describes our approach to state machine abstraction and compliance checking using the alignment technique. The approach requires an event log and a defined state machine as inputs, and consists of three components: annotation, abstraction, and checker.\n",
    "\n",
    "The first component, the annotation component, is responsible for annotating the events contained in an event log based on a knowledge base. This annotation process identifies whether an event is related to a state or a transition in order to abstract a state machine discovered in the next component.\n",
    "\n",
    "The second component, the abstraction component, is responsible for discovering a state machine in the event log. This component uses the annotated event log to identify the states and transitions in the log and to construct a state machine model on the basis of patterns.\n",
    "\n",
    "The last component, the checker, compares the discovered state machine with the defined state machine to identify deviations between what happened and what is defined. This component checks for compliance between the defined state machine and the actual behavior captured in the event log.\n",
    "\n",
    "Overall, the approach presented in this notebook provides a way to automatically abstract state machines from event logs and to check their compliance with a defined state machine. \n",
    "\n",
    "## Annotation ##\n",
    "\n",
    "The first component use an ontology for describing the Knowledge Base and annotate the events composing the event log. We use python and the library owlready2 to implement and read ontology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import * \n",
    "onto = get_ontology('http://test.org/onto.owl')\n",
    "\n",
    "##### Ontology Classes Definition ##### \n",
    "with onto: \n",
    "    class stateMachine(Thing):\n",
    "        pass\n",
    "    class State(stateMachine):\n",
    "        pass\n",
    "    class Start(State):\n",
    "        pass\n",
    "    class Execute(State):\n",
    "        pass\n",
    "    class Complete(State):\n",
    "        pass\n",
    "    class Transition(stateMachine):\n",
    "        pass\n",
    "    class eventType(Thing):\n",
    "        pass\n",
    "    class isRelatedTo(ObjectProperty, FunctionalProperty):\n",
    "        domain = [eventType]\n",
    "        range  = [stateMachine]\n",
    "\n",
    "##### Declare event types ##### Individuals\n",
    "Service_Create = eventType('Service_Create', isRelatedTo=Start)\n",
    "Service_Remove = eventType('Service_Remove', isRelatedTo=Start)\n",
    "Service_Update = eventType('Service_Update', isRelatedTo=Start)\n",
    "Container_Create = eventType('Container_Create', isRelatedTo=Execute)\n",
    "Container_Destroy = eventType('Container_Destroy', isRelatedTo=Execute)\n",
    "Container_Start = eventType('Container_Start', isRelatedTo=Complete)\n",
    "Container_Stop = eventType('Container_Stop', isRelatedTo=Complete)\n",
    "Ressource_Usage = eventType('Ressource_Usage', isRelatedTo=Transition)\n",
    "\n",
    "onto.save(file='onto.owl', format=\"rdfxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.stateMachine, onto.State, onto.Start, onto.Execute, onto.Complete, onto.Transition, onto.eventType]\n",
      "[onto.Service_Create, onto.Service_Remove, onto.Service_Update, onto.Container_Create, onto.Container_Destroy, onto.Container_Start, onto.Container_Stop, onto.Ressource_Usage]\n",
      "[onto.isRelatedTo]\n"
     ]
    }
   ],
   "source": [
    "print(list(onto.classes()))\n",
    "print(list(onto.individuals()))\n",
    "print(list(onto.object_properties()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a function to find the ancestors in an ontology of the type of event identified in the event logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Ancestors Calling #####\n",
    "def search_ancestors(onto, ask):\n",
    "    result = onto.search(iri = \"*{}\".format(ask))\n",
    "    lcStep = str(result[0].isRelatedTo).split('.')[1]\n",
    "    smElt = str(result[0].isRelatedTo.is_a[0]).split('.')[1]\n",
    "    if lcStep == 'Transition':\n",
    "        smElt = 'Transition'\n",
    "        lcStep = 'N/A'\n",
    "    return [smElt, lcStep]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotation function of the event-logs is implemented in the final stage, whereby the state-machine element and lifecycle step are incorporated into each event. The function outputs an XES file containing the annotated events with the additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw event-logs\n",
      "                   Timestamp     Source Resource Name         Event-Type     Metric Value\n",
      "0  2023-03-13 00:00:03+00:00   Provider          Auth     Service_Create   replicas     2\n",
      "1  2023-03-13 00:00:04+00:00   Provider          Auth   Container_Create          /     /\n",
      "2  2023-03-13 00:00:05+00:00   Provider          Auth    Container_Start          /     /\n",
      "3  2023-03-13 00:00:05+00:00  Ressource          Auth    Ressource_Usage  Cpu Usage    15\n",
      "4  2023-03-13 00:01:05+00:00  Ressource          Auth    Ressource_Usage  Cpu Usage    15\n",
      "..                       ...        ...           ...                ...        ...   ...\n",
      "73 2023-03-13 00:03:05+00:00  Ressource            UI    Ressource_Usage  Cpu Usage    15\n",
      "74 2023-03-13 00:03:15+00:00  Ressource            UI    Ressource_Usage  Cpu Usage    15\n",
      "75 2023-03-13 00:03:45+00:00   Provider            UI     Service_Update   replicas     0\n",
      "76 2023-03-13 00:03:46+00:00   Provider            UI     Container_Stop          /     /\n",
      "77 2023-03-13 00:03:47+00:00   Provider            UI  Container_Destroy          /     /\n",
      "\n",
      "[78 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmechouch\\AppData\\Local\\Temp\\ipykernel_15996\\1118936225.py:8: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  dataframe = pm4py.format_dataframe(dataframe, case_id='Resource Name', activity_key='Event-Type', timestamp_key='Timestamp')\n"
     ]
    }
   ],
   "source": [
    "#### Pre-processing based on ontology ####\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "\n",
    "#### Import event-logs from CSV to pandas dataframe\n",
    "dataframe = pd.read_csv('logs.csv', sep=',')\n",
    "pd.set_option('display.width',1000)\n",
    "dataframe = pm4py.format_dataframe(dataframe, case_id='Resource Name', activity_key='Event-Type', timestamp_key='Timestamp')\n",
    "\n",
    "### Print Raw event logs\n",
    "print(\"Raw event-logs\")\n",
    "print(dataframe[['Timestamp', 'Source', 'Resource Name', 'Event-Type', 'Metric', 'Value']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmechouch\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pm4py\\utils.py:486: UserWarning: the EventLog class has been deprecated and will be removed in a future release.\n",
      "  warnings.warn(\"the EventLog class has been deprecated and will be removed in a future release.\")\n",
      "c:\\Users\\jmechouch\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "exporting log, completed traces :: 100%|██████████| 3/3 [00:00<00:00, 285.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated logs with state-machine element (smElt) and lifecycle step (lcStep)\n",
      "                   Timestamp     Source Resource Name         Event-Type     Metric Value       smElt    lcStep\n",
      "0  2023-03-13 00:00:03+00:00   Provider          Auth     Service_Create   replicas     2       State     Start\n",
      "1  2023-03-13 00:00:04+00:00   Provider          Auth   Container_Create          /     /       State   Execute\n",
      "2  2023-03-13 00:00:05+00:00   Provider          Auth    Container_Start          /     /       State  Complete\n",
      "3  2023-03-13 00:00:05+00:00  Ressource          Auth    Ressource_Usage  Cpu Usage    15  Transition       N/A\n",
      "4  2023-03-13 00:01:05+00:00  Ressource          Auth    Ressource_Usage  Cpu Usage    15  Transition       N/A\n",
      "..                       ...        ...           ...                ...        ...   ...         ...       ...\n",
      "73 2023-03-13 00:03:05+00:00  Ressource            UI    Ressource_Usage  Cpu Usage    15  Transition       N/A\n",
      "74 2023-03-13 00:03:15+00:00  Ressource            UI    Ressource_Usage  Cpu Usage    15  Transition       N/A\n",
      "75 2023-03-13 00:03:45+00:00   Provider            UI     Service_Update   replicas     0       State     Start\n",
      "76 2023-03-13 00:03:46+00:00   Provider            UI     Container_Stop          /     /       State  Complete\n",
      "77 2023-03-13 00:03:47+00:00   Provider            UI  Container_Destroy          /     /       State   Execute\n",
      "\n",
      "[78 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Iterate through event logs ####\n",
    "for idx, row in dataframe.iterrows():\n",
    "    # Search event type in ontology and returns ancestors \n",
    "    smElt, lcStep = search_ancestors(onto, row['Event-Type'])\n",
    "    dataframe.loc[[idx],'smElt'] = smElt\n",
    "    dataframe.loc[[idx],'lcStep'] = lcStep\n",
    "\n",
    "### Export as XES ###\n",
    "event_log = pm4py.convert_to_event_log(dataframe)\n",
    "xes = pm4py.write_xes(event_log, 'exported.xes')\n",
    "\n",
    "### Print Dataframe\n",
    "print(\"Annotated logs with state-machine element (smElt) and lifecycle step (lcStep)\")\n",
    "print(dataframe[['Timestamp', 'Source', 'Resource Name', 'Event-Type', 'Metric', 'Value', 'smElt', 'lcStep']])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the annotation component generates an annotated XES file that includes state-machine elements (smElt) and lifecycle steps (lcStep). This file is then utilized by the subsequent component responsible for state-machine abstraction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstraction\n",
    "\n",
    "Based on the XES annotated event logs, we abstract state-machine using our defined patterns. We define several functions in order to identify state-machine elements. We also define an abstract function for pattern identification of events in a pandas dataframe rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function Definition ####\n",
    "import pm4py\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "from StateMachine import StateMachine\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "def pattern_identification(log: pd.DataFrame, pattern: List, attribute: str):\n",
    "    \"\"\" \n",
    "        Return index of pattern in log in DataFrame \n",
    "        To Do: \n",
    "            - Add functionnalities to define patterns across several attributes\n",
    "            - Enabled possibilities of eventually follows pattern \n",
    "    \"\"\"\n",
    "    # Identify number of item in pattern\n",
    "    nbPattern = len(pattern)\n",
    "\n",
    "    # Construction pattern as string\n",
    "    ## Begin of request\n",
    "    pattern_s = f\"\"\"log.index[(log['{attribute}'] == '{pattern[0]}')\"\"\"\n",
    "    for item in range(1, nbPattern):\n",
    "        pattern_s += f\"\"\" & (log['{attribute}'].shift(-{item}) == '{pattern[item]}')\"\"\"\n",
    "    ## End of request\n",
    "    pattern_s += f\"\"\"]\"\"\"\n",
    "\n",
    "    # Execution of defined pattern\n",
    "    indice_p_s = eval(pattern_s)\n",
    "    return indice_p_s\n",
    "\n",
    "def state_abstraction(log: pd.DataFrame):\n",
    "    \"\"\"\n",
    "        Return states identified\n",
    "    \"\"\"\n",
    "    ## Declare discovered state machine\n",
    "    SM_Discovered = StateMachine(\n",
    "        name=''\n",
    "    )\n",
    "    pattern=['Start', 'Execute', 'Complete']\n",
    "\n",
    "    states_index = pattern_identification(log, pattern, 'lcStep')\n",
    "    states_name = []\n",
    "\n",
    "    for i, s in enumerate(states_index, 1):\n",
    "        S_name = 'S'+str(i)\n",
    "        states_name.append(S_name)\n",
    "        SM_Discovered.add_state(StateMachine.state(\n",
    "            name= S_name,\n",
    "            type='',\n",
    "            Resourcerequirements={\n",
    "                log.loc[s]['Metric'] : log.loc[s]['Value']\n",
    "            }\n",
    "        ))\n",
    "    return SM_Discovered, states_index, states_name\n",
    "\n",
    "def state_type_abstraction(log: pd.DataFrame, State_Machine_Discovered: StateMachine):\n",
    "    \"\"\"\n",
    "        Apply State-Type Abstraction\n",
    "    \"\"\"\n",
    "    state_nb = len(State_Machine_Discovered.states)\n",
    "    for state in State_Machine_Discovered.states:\n",
    "        if state.name == 'S1':\n",
    "            state.set_type('isInitial')\n",
    "        elif int(state.name[1:]) < state_nb:\n",
    "            state.set_type('isNormal')\n",
    "        elif int(state.name[1:]) == state_nb:\n",
    "            state.set_type('isFinal')\n",
    "    return State_Machine_Discovered\n",
    "\n",
    "def transition_abstraction(log: pd.DataFrame, State_Machine_Discovered: StateMachine, states_index):\n",
    "    \"\"\"\n",
    "        Abstraction transition by combining reconfiguration actions and triggering event associated to the state-machine\n",
    "\n",
    "    \"\"\"\n",
    "    states = State_Machine_Discovered.states\n",
    "    for idx, state in enumerate(states):\n",
    "        if idx < (len(states) - 1):\n",
    "            diff_state = int(states[idx+1].Resourcerequirements['replicas']) - int(states[idx].Resourcerequirements['replicas'])\n",
    "\n",
    "            if diff_state > 0:\n",
    "                type = 'Scale-out'\n",
    "            elif diff_state < 0:\n",
    "                type = 'Scale-in'\n",
    "            else:\n",
    "                type = 'Error'\n",
    "                print('Error: State Equivalent')\n",
    "\n",
    "            #### Get states Event\n",
    "            # Set time window selected\n",
    "            time_window = timedelta(minutes=1)\n",
    "\n",
    "            # Select events in the time window before state execution\n",
    "            pattern_ts = log.loc[states_index[idx+1]]['time:timestamp']\n",
    "            pattern_ts_minus_tw = (pattern_ts - time_window).isoformat()\n",
    "            transition_Window = log[ ( log['time:timestamp'] > pattern_ts_minus_tw) & \\\n",
    "                (log['time:timestamp'] < pattern_ts) & (log['smElt'] == 'Transition' )].astype({'Value': int})\n",
    "            \n",
    "            # Return for each metric observed a consumption average\n",
    "            avg = transition_Window.groupby('Metric')['Value'].mean().to_dict()\n",
    "\n",
    "            if bool(avg) != False:\n",
    "                State_Machine_Discovered.add_transition(\n",
    "                    StateMachine.transition(\n",
    "                        name=f\"T{idx+1}\",\n",
    "                        source=state.name,\n",
    "                        target=states[idx+1].name,\n",
    "                        events=[StateMachine.event(\n",
    "                                    id = 'E1',\n",
    "                                    type = 'ResourceRelatedEvent',\n",
    "                                    predicate = {\n",
    "                                        'metric': 'Cpu Usage',\n",
    "                                        'operator': '>=',\n",
    "                                        'refValue': avg['Cpu Usage'],\n",
    "                                        'time': str(time_window.total_seconds()) + 's'\n",
    "                                    })],\n",
    "                        actions=[StateMachine.action(\n",
    "                            id = 'A1',\n",
    "                            type = type,\n",
    "                            attributes= {\n",
    "                                'replicas' : abs(diff_state)\n",
    "                            }\n",
    "                        )]\n",
    "                ))\n",
    "\n",
    "    return State_Machine_Discovered\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution of Abstraction \n",
    "\n",
    "Using these defined abstraction pattern, we identified state-machine from the event-logs. Then, we show the identified state-machine. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 3/3 [00:00<00:00, 341.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: [ (S1, isInitial, {'replicas': '2'}), (S2, isNormal, {'replicas': '4'}), (S3, isNormal, {'replicas': '6'}), (S4, isFinal, {'replicas': '8'}) ]\n",
      "Transitions: [ ('T1':'S1'->'S2',[(E1,ResourceRelatedEvent,{'metric': 'Cpu Usage', 'operator': '>=', 'refValue': 15.0, 'time': '60.0s'})],[(A1,Scale-out,{'replicas': 2})]), ('T2':'S2'->'S3',[(E1,ResourceRelatedEvent,{'metric': 'Cpu Usage', 'operator': '>=', 'refValue': 15.0, 'time': '60.0s'})],[(A1,Scale-out,{'replicas': 2})]), ('T3':'S3'->'S4',[(E1,ResourceRelatedEvent,{'metric': 'Cpu Usage', 'operator': '>=', 'refValue': 11.666666666666666, 'time': '60.0s'})],[(A1,Scale-out,{'replicas': 2})]) ]\n",
      "States: [ (S1, isInitial, {'replicas': '2'}), (S2, isFinal, {'replicas': '8'}) ]\n",
      "Transitions: [ ('T1':'S1'->'S2',[(E1,ResourceRelatedEvent,{'metric': 'Cpu Usage', 'operator': '>=', 'refValue': 15.0, 'time': '60.0s'})],[(A1,Scale-out,{'replicas': 6})]) ]\n",
      "States: [ (S1, isInitial, {'replicas': '2'}), (S2, isNormal, {'replicas': '4'}), (S3, isFinal, {'replicas': '6'}) ]\n",
      "Transitions: [ ('T1':'S1'->'S2',[(E1,ResourceRelatedEvent,{'metric': 'Cpu Usage', 'operator': '>=', 'refValue': 15.0, 'time': '60.0s'})],[(A1,Scale-out,{'replicas': 2})]), ('T2':'S2'->'S3',[(E1,ResourceRelatedEvent,{'metric': 'Cpu Usage', 'operator': '>=', 'refValue': 15.0, 'time': '60.0s'})],[(A1,Scale-out,{'replicas': 2})]) ]\n"
     ]
    }
   ],
   "source": [
    "##### Importation of annotated event logs #####\n",
    "file_path = 'exported.xes'\n",
    "event_log = pm4py.read_xes(file_path)\n",
    "\n",
    "# Filter by case\n",
    "events = event_log.groupby('@@case_index')\n",
    "for i, case_event_log in events:\n",
    "    ##### State abstraction : Pattern 3.1 #####\n",
    "    SM_Discovered, states_index, states_name = state_abstraction(case_event_log)\n",
    "\n",
    "    ##### State-Type abstraction : Pattern 3.2 #####\n",
    "    SM_Discovered = state_type_abstraction(case_event_log, SM_Discovered)\n",
    "\n",
    "    ##### Transition abstraction : Pattern 3.3 + 3.4 #####\n",
    "    SM_Discovered = transition_abstraction(case_event_log, SM_Discovered, states_index)\n",
    "\n",
    "    json = SM_Discovered.to_json()\n",
    "\n",
    "    print(SM_Discovered)\n",
    "\n",
    "    with open(\"SM_discovered/SM_.json\", \"w\") as outfile:\n",
    "        outfile.write(json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the discovered state-machine in hand, it is now possible to apply conformance checking with the defined state-machine. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment\n",
    "\n",
    "The final section of this study focuses on the implementation of an alignment algorithm that is used to identify any deviations between the defined state-machine and the observed state-machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Discovered state-machine ###\n",
      "['S1', 'S2', 'S3']\n",
      "[('S1', 'S2'), ('S2', 'S3')]\n",
      "### Defined state-machine ###\n",
      "['S1', 'S2', 'S3']\n",
      "[('S1', 'S2'), ('S2', 'S3')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Checker \n",
    "\"\"\" \n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "def to_graph(SM):\n",
    "    SM_disc = nx.DiGraph()\n",
    "    for state in SM['_StateMachine__states']:\n",
    "        SM_disc.add_node(state['_state__name'], replicas = state['_state__Resourcerequirements']['replicas'], type=state['_state__type'])\n",
    "    for transition in SM['_StateMachine__transitions']:\n",
    "        SM_disc.add_edge(transition['_transition__source'], transition['_transition__target'], \n",
    "                         name=transition['_transition__name'], events=transition['_transition__events'], actions=transition['_transition__actions'])\n",
    "    return SM_disc\n",
    "\n",
    "def get_initial_nodes(graph):\n",
    "    return [n for n,d in graph.in_degree() if d==0]\n",
    "\n",
    "def get_final_nodes(graph):\n",
    "    return [n for n,d in graph.out_degree() if d==0]\n",
    "\n",
    "### Discovered state-machine\n",
    "SM_Disc = to_graph(json.load(open(\"SM_discovered/SM_.json\")))\n",
    "\n",
    "### Defined state-machine\n",
    "SM_Def = to_graph(json.load(open(\"SM_Defined.json\")))\n",
    "\n",
    "print(\"### Discovered state-machine ###\")\n",
    "print(SM_Disc.nodes)\n",
    "print(SM_Disc.edges)\n",
    "\n",
    "print(\"### Defined state-machine ###\")\n",
    "print(SM_Def.nodes)\n",
    "print(SM_Def.edges)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Space Construction\n",
    "\n",
    "The alignment technique is utilized to identify deviations by attempting to align the defined state-machine and the discovered state-machine. The process of searching for the optimal alignment is carried out in two steps: first, constructing a search space that represents all possible moves; and second, searching for the optimal path within this search space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, \"['S1', 'S1']\", \"['S2', 'S2']\", \"['S3', 'S3']\"]\n",
      "[(0, \"['S1', 'S1']\"), (\"['S1', 'S1']\", \"['S2', 'S2']\"), (\"['S2', 'S2']\", \"['S3', 'S3']\")]\n"
     ]
    }
   ],
   "source": [
    "### Search Space construction\n",
    "SS = nx.DiGraph()\n",
    "SS.add_node(0, weight=0)\n",
    "\n",
    "for i, (eltx, elty) in enumerate(zip(SM_Disc.nodes, SM_Def.nodes)):\n",
    "    temp_last_nodes = get_final_nodes(SS)\n",
    "    # e = epsilon to guarantee end\n",
    "    e = i*0.1\n",
    "    if SM_Disc.nodes[eltx]['replicas'] == SM_Def.nodes[eltx]['replicas']:\n",
    "        # State equivalent \n",
    "        SS.add_node(str([eltx,elty]), weight=1+e)\n",
    "        [SS.add_edge(node, str([eltx,elty])) for node in temp_last_nodes]\n",
    "    else:\n",
    "        SS.add_node(str([eltx,'>>']), weight=5+e)\n",
    "        SS.add_node(str(['>>',elty]), weight=5+e)\n",
    "        [SS.add_edge(node, str([eltx,'>>'])) for node in temp_last_nodes]\n",
    "        [SS.add_edge(node, str(['>>',elty])) for node in temp_last_nodes]\n",
    "\n",
    "print(SS.nodes)\n",
    "print(SS.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report : \n",
      "Alignment : [([0, \"['S1', 'S1']\", \"['S2', 'S2']\", \"['S3', 'S3']\"], 3.3)]\n",
      "Y_Optimal : 3.3\n",
      "FitnessValue : 0.89\n"
     ]
    }
   ],
   "source": [
    "### Identify starting and ending nodes of the search space\n",
    "starting_nodes = get_initial_nodes(SS)\n",
    "ending_nodes = get_final_nodes(SS)\n",
    "\n",
    "# Compute the worst possible alignment\n",
    "y_worst_sum = ((len(SM_Def.nodes) * 2 ) * 5)\n",
    "\n",
    "### Compute the cost of an identified alignment \n",
    "results_path = []\n",
    "for s in starting_nodes:\n",
    "    for e in ending_nodes:\n",
    "        y_optimal_cost = 0\n",
    "        path = nx.astar_path(SS, s, e)\n",
    "        for elt in path: y_optimal_cost+=SS.nodes[elt]['weight']\n",
    "        results_path.append((path, y_optimal_cost))\n",
    "        fitnessValue = 1 - y_optimal_cost / y_worst_sum\n",
    "\n",
    "print(\"Report : \")\n",
    "print(f\"Alignment : {results_path}\")\n",
    "print(f\"Y_Optimal : {y_optimal_cost}\")\n",
    "print(f\"FitnessValue : {fitnessValue}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d1ea9fb40c093137e5631ea40405a634864414780e4b5725c09d6f101604b5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
